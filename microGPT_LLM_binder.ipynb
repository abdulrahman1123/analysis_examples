{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f9a4f013",
      "metadata": {
        "id": "f9a4f013"
      },
      "source": [
        "# Language Models\n",
        "\n",
        "In our pursuit of making computers understand and generate human-like text, LLM were developed.\n",
        "Language modeling in general is concerned with predicting the next word in a sequence of words. One of the early and basic examples of LMs is the n-gram model, where the probability of a word occurring is calculated based on the previous n-1 words.\n",
        "\n",
        "For example, in a 5-gram model, and the sentence \"The quick brown fox jumps over the lazy dog\", the probability of the word \"dog\" occuring is calculated based on the previous 4 words \"over the lazy\". This is a very simple model and does not capture the context of the sentence very well.\n",
        "\n",
        "<img src=\"https://www.baeldung.com/wp-content/uploads/sites/4/2023/05/Language-Model-N-gram.jpg\" />\n",
        "\n",
        "source: https://www.baeldung.com/cs/large-language-models\n",
        "\n",
        "## Large Language Models\n",
        "The benifit of N-gram models is limited. This is where large language models (LLMs) come in. LLMs are able to capture the context of the sentence and generate more accurate predictions. They language models that use large neural networks with up to billions of parameters. They can capture more complex syntax and semantics of the human language. They can perform a number of tasks, ranging from translation, identifying offensive content, or even passing professional exams.\n",
        "\n",
        "To further understand LLMs, we will introduce two concepts here, **embedding** and **tokenization**.\n",
        "\n",
        "### Embeddings\n",
        "You can think of embeddings as values assigned to words that represent aspects of their meaning. For example, if we want to describe objects in terms of their size and taste, a dog will score high on the size and low on the taste, while a rock will score low on both, and so on. When the model learns such values, it acquires some form of representation of word *meaning*. That is why, **embeddings are a major part of what the models are actually trained on**.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/abdulrahman1123/analysis_examples/refs/heads/main/embeddings.png\" width=400 height=423 />\n",
        "\n",
        "### Tokenization and Encoding\n",
        "Have you ever subscriped to the 'pay as you go' tier in ChatGPT, Cluade or Deepseek? If so, then you probably noticed that they calculate usage by the number of tokens. But what are tokens?\n",
        "Well, they are the smallest chunks that you split your text into. Usually, they represent sub-word syllables. For example, the word \"unbelievable\" can consist of three tokens, \"un\", \"believ\" and \"able\". However, we can determine the granularity of our definition of tokens, and we can split our data on the letter level, or on the whole-word level if we want to.\n",
        "\n",
        "Finally, we all know how computers love numbers. So, we want to convert all words to numbers before feeding them to the model for training. This is known as **encoding**. When we get the output of the machine, we convert it back to words using **decoding**.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/abdulrahman1123/analysis_examples/refs/heads/main/tokenization.png\" width=1050 height=225 />\n",
        "\n",
        "Enough talk now. let's start coding.\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66c45cfb",
      "metadata": {},
      "source": [
        "# Writing an LLM\n",
        "### importing important functions and libraries\n",
        "I created a file containing all important functions that we need. All we have to do now is clone my repository by running the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j7Wm1rtQqrtl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "id": "j7Wm1rtQqrtl",
        "outputId": "890d0e60-4095-4683-f716-6cee3cf7996e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'analysis_examples'...\n",
            "remote: Enumerating objects: 664, done.\u001b[K\n",
            "remote: Counting objects: 100% (168/168), done.\u001b[K\n",
            "remote: Compressing objects: 100% (111/111), done.\u001b[K\n",
            "remote: Total 664 (delta 98), reused 115 (delta 52), pack-reused 496 (from 1)\u001b[K\n",
            "Receiving objects: 100% (664/664), 32.66 MiB | 34.98 MiB/s, done.\n",
            "Resolving deltas: 100% (378/378), done.\n"
          ]
        }
      ],
      "source": [
        "# Import important functions, including functions to build the model and train it\n",
        "from microGPT import *\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import requests\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
        "import platform\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34da005f",
      "metadata": {},
      "source": [
        "## Shakespeare's Work\n",
        "Let's download theentirety of Shakespeare's work to train our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6c18a715",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of dataset in characters: 5376417\n",
            "THE SONNETS\n",
            "\n",
            "                    1\n",
            "\n",
            "From fairest creatures we desire increase,\n",
            "That thereby beauty’s rose might never die,\n",
            "But as the riper should by time decease,\n",
            "His tender heir might bear his memory:\n",
            "But thou contracted to thine own bright eyes,\n",
            "Feed’st thy light’s flame with self-substantial fuel,\n"
          ]
        }
      ],
      "source": [
        "url1 = \"https://www.gutenberg.org/cache/epub/100/pg100.txt\"\n",
        "text = requests.get(url1).text.replace('\\r','')\n",
        "text = text[text.find('THE SONNETS\\n\\n')::] # remove text introduction\n",
        "print(f\"Length of dataset in characters: {len(text)}\")\n",
        "print(text[:302])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d473a959",
      "metadata": {},
      "source": [
        "### Hyperparameters\n",
        "Hyperparameters are parameters that specify how the model is built and trained. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "Hy6pLTNuq4EQ",
      "metadata": {
        "id": "Hy6pLTNuq4EQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "batch_size = 16 # how many independent sequences will we process in parallel\n",
        "block_size = 64 # what is the maximum context length for predictions\n",
        "max_iters = 1000\n",
        "learning_rate = 2e-4\n",
        "device = torch.device(\"cpu\")\n",
        "eval_iters = 100\n",
        "eval_interval = max_iters//10\n",
        "n_embd = 128\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "n_embd = (n_embd//n_head)*n_head\n",
        "dropout = 0.1\n",
        "vocab_size = 10000\n",
        "\n",
        "\n",
        "max_iters = 10000\n",
        "learning_rate = 2e-4\n",
        "device = torch.device(\"mps\")\n",
        "eval_iters = 100\n",
        "eval_interval = max_iters//10\n",
        "batch_size = 32\n",
        "vocab_size = 20000\n",
        "n_embd = 256\n",
        "block_size = 256\n",
        "n_head = 8\n",
        "n_layer=8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a69c484",
      "metadata": {},
      "source": [
        "### Create Tokenizer, Encoder and Decoder functions\n",
        "create Tokenizer, Encoder and Decoder functions. The encoder is used to feed encoded data to the model (numbers only), and the decoder is used to convert the model's output into characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "6660e4e9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Vocab size: 20000\n",
            " Gre ----> 3894\n",
            "et ----> 165\n",
            "ings ----> 519\n",
            ", ----> 12\n",
            " My ----> 1308\n",
            " king ----> 734\n",
            "! ----> 4\n",
            " ----> 3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#create tokens\n",
        "encode,decode, tokenizer, vocab_size = tokenize(text, vocab_size)\n",
        "\n",
        "# check what it does\n",
        "encoding = encode('Greetings, My king!')\n",
        "for item in encoding[1::]:\n",
        "    print(f\"{decode([item])} ----> {item}\")\n",
        "\n",
        "\n",
        "# Encode the entire dataset\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89506964",
      "metadata": {},
      "source": [
        "### Training and Testing\n",
        "I guess you are now familiar with the concept of training and testing. I wrote a simple function that splits the data into 90% training and 10% testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "8eccc907",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size = 1341850 tokens ... Validation size = 149100 tokens\n"
          ]
        }
      ],
      "source": [
        "# split into training and testing\n",
        "train_data, val_data = train_test_split(data, 0.9)\n",
        "train_data, val_data = train_data.to(device), val_data.to(device)\n",
        "print(f'Train size = {train_data.shape[0]} tokens ... Validation size = {val_data.shape[0]} tokens')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "317d5faf",
      "metadata": {},
      "source": [
        "### Creating and Training the Model\n",
        "Now we want to build the model and some optimizing functions, and then train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "cc300ef2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 10.0957, val loss 10.0986\n",
            "step 1000: train loss 5.5660, val loss 5.8031\n",
            "step 2000: train loss 5.2206, val loss 5.5287\n",
            "step 3000: train loss 5.0201, val loss 5.3827\n",
            "step 4000: train loss 4.8708, val loss 5.3296\n",
            "step 5000: train loss 4.7575, val loss 5.3113\n",
            "step 6000: train loss 4.6760, val loss 5.2831\n",
            "step 7000: train loss 4.6066, val loss 5.2628\n",
            "step 8000: train loss 4.5682, val loss 5.2673\n",
            "step 9000: train loss 4.5689, val loss 5.2664\n",
            "step 9999: train loss 4.5648, val loss 5.2629\n",
            "Model saved as: model_batch32_vocab20000_nembed256_block256_nhead8_n_layer8.pt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "model = BigramLanguageModel(vocab_size, n_embd, block_size, n_head, dropout, n_layer, device).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=get_lr_lambda(max_iters=max_iters, warmup_steps=200))\n",
        "\n",
        "# train the model\n",
        "train_model(model, max_iters, eval_iters, train_data, val_data, batch_size,eval_interval, block_size,device,optimizer,scheduler)\n",
        "\n",
        "base_dir = \"/Users/abdelrahmansawalma/Downloads/LLMs\"\n",
        "model_name = f'model_batch{batch_size}_vocab{vocab_size}_nembed{n_embd}_block{block_size}_nhead{n_head}_n_layer{n_layer}'\n",
        "model_path = os.path.join(base_dir,f'{model_name}.pt')\n",
        "#save the model\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved as: {model_name}.pt\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f22bbba",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "eb9c9350",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model downloaded successfully as model.pth\n",
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "model_url = \"https://www.coreunitrdm.biozentrum.uni-wuerzburg.de/public.php/dav/files/G9NdHrd9F8wH39r/?accept=zip\"\n",
        "\n",
        "\n",
        "# Download the model file\n",
        "def download_model(url, local_path=\"model.pth\"):\n",
        "    # Try direct download first\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(local_path, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Model downloaded successfully as {local_path}\")\n",
        "        return local_path\n",
        "    else:\n",
        "        print(f\"Download failed with status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Download the model\n",
        "local_model_path = download_model(model_url)\n",
        "\n",
        "model = BigramLanguageModel(20000, 256, 256, 8, 0.1, 8, device)\n",
        "model.load_state_dict(torch.load(local_model_path, map_location=device))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6958a677",
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"To be, or not to be \"\n",
        "input_ids = torch.tensor([tokenizer.encode(prompt).ids], dtype=torch.long, device=device)\n",
        "out = model.generate(idx=input_ids, max_new_tokens=200)\n",
        "print(tokenizer.decode(out[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "61adb228",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Vocab size: 20000\n",
            " To be, or not to be 8 to me ’)ke thrust they pl�Romans)ar hissght,��ott)Empaitingnter stirsThan Volumnius,��ott,� kindness th Cadwal we,Comoose we thame!��ott,�ThouThen hate they�asionsittleming lightning intents,� Step! succipe gHUMUS,��ANTUS,� hornsVERG vision,� passages CLOWN!��ott,� arllp wme mean to writeDEMETRIUSid,��ott,\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "base_dir = \"/Users/abdelrahmansawalma/Downloads/LLMs\"\n",
        "batch_size = 32\n",
        "vocab_size = 20000\n",
        "n_embd = 256\n",
        "block_size = 256\n",
        "n_head = 8\n",
        "n_layer=8\n",
        "encode,decode, tokenizer, vocab_size = tokenize(text, vocab_size)\n",
        "\n",
        "model_name = f'model_batch{batch_size}_vocab{vocab_size}_nembed{n_embd}_block{block_size}_nhead{n_head}_n_layer{n_layer}'\n",
        "model_path = os.path.join(base_dir,f'{model_name}.pt')\n",
        "\n",
        "\n",
        "##load the model\n",
        "model = BigramLanguageModel(vocab_size, n_embd, block_size, n_head, dropout, n_layer, device)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "prompt = \"To be, or not to be \"\n",
        "input_ids = torch.tensor([tokenizer.encode(prompt).ids], dtype=torch.long, device=device)\n",
        "out = model.generate(idx=input_ids, max_new_tokens=100)\n",
        "print(tokenizer.decode(out[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e397968",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5376417\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "77bc070c",
      "metadata": {
        "id": "77bc070c"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ad08802d",
      "metadata": {
        "id": "ad08802d"
      },
      "source": [
        "---\n",
        "### Sources:\n",
        "[Introduction to Large Language Models](https://www.baeldung.com/cs/large-language-models)\n",
        "\n",
        "[Youtube video about writing a GPT from scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=2409s)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e747ec88",
      "metadata": {
        "id": "e747ec88"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
