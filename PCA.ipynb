{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "The aim of this tutorial is to explain how PCA works, how to interpret the results and how you can use it in your research. Hopefully in a simplified way.\n",
    "\n",
    "PCA is a dimensionality reduction method often used with large data frames. This means that if you have a data frame with large number of variables, PCA will be very helpful in **reducing the number of variables, while retaining most of the information**. Although this might lead to some data loss, the benefit of PCA is that you can trade some accuracy for much improved simplicity, which has its benefits as we will see. \n",
    "\n",
    "### What does PCA represent?\n",
    "\n",
    "The main aim of PCA is to find components that hold as much information as possible. The first component holds most information, followed by the second and so on. You can think of the first component as camera that tries to find the best shot to show most of the data. The second component will be a second camera trying to find the second best shot (that is perpindicular to the first one), and so on.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/abdulrahman1123/analysis_examples/main/pca_intro_1.png\" width=1100 height=400 />\n",
    "\n",
    "### But how do we actually compute it? ... Eigenvalues?! what is that?\n",
    "\n",
    "To compute PCA that we need to find the axis that if you project the data onto, you would have the most variance possible (which means most information possible). Thankfully, there is a simple way to do it, which is by finding the eigenvalues and eigenvectors of your data.  Don't worry, I know I said 'simplified way' above, but I promise this will be simplified too. \n",
    "\n",
    "Simply put, eigenvectors describe the direction of each component. In other words, it is the best direction of a line that if you project the data onto, you capture most of the variance (and hence most of the information) while having the lowest possible distance from that line. \n",
    "In addition to the direction, we need a description of the value of each vector, or how important it is. This is where eigenvalues come into play: the higher the eigenvalue of each eigenvector, the more variance it holds, and the more information it contains.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/abdulrahman1123/analysis_examples/main/pca_intro_2.png\" width=1300 height=450 />\n",
    "\n",
    "If we plot the resulting PCs, we get the following:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/abdulrahman1123/analysis_examples/main/pcs_plot.png\" width=400 height=320 />\n",
    "\n",
    "In this example above, the proportion of explained variance (proportion of information contained) for the two components are 96% and 4%. This means that we can safely discard of the second PC and describe our data using the first component only. But more on that later.\n",
    "\n",
    "### What are its benefits and why do we need to reduce the number of variables?\n",
    "- Get rid of useless variables: some variables do not add any meaningful information to your study. These might be particular EEG channels that do not pick your signal of interest or a set of genes not related to the character you are studying ... etc.\n",
    "- Better visualization: Imagine you have 200,000 genes you want to plot to show their effect on particular trait.\n",
    "- Avoiding redundency: Sometimes you measure the similar information using different questionnaires. You want one variable that represents each set of similar variables, without redundency and while retaining most information.\n",
    "- Other: such as speeding up machine learning algorithms, better interpreting correlations between variables\n",
    "\n",
    "You can check [this blog](https://glowingpython.blogspot.com/2011/07/pca-and-image-compression-with-numpy.html) to see how image information can be reduced using 40 components, producing an image very close to original with only 40 components\n",
    "\n",
    "### Disadvantages:\n",
    "- Lose information: You need to keep this in mind. If the reduction is not significant and your variables do not carry much shared information, you might need to consider using all variables without reduction\n",
    "- Heavily affected by differences in variance (Thus, you will **always** need to normalize your data)\n",
    "\n",
    "### Did someone say \"PC loading\"?\n",
    "Finally, although PCs are purely mathematical entities, it would be very helpful if you can describe your components. This will make any analysis you do more interpretable and easier to understand. To do this, you need to know the contribution of each variable in the original data to each of the PCs. No need to worry, most software packages provide this as well. For example, if you have a data set about anxiety patients, and you have the following variables: age, anxiety level, harm avoidance level, worry level and quality of life, and you used PCA to get your components. Then you looked at the importance (or contribution or loading) of your components and found this:\n",
    "- PC#1 → anxiety level, harm avoidance level, worry level\n",
    "- PC#2 → age\n",
    "- PC#3 → quality of life\n",
    "\n",
    "Then, you can *roughly* call the first component \"Anxiety\", the second \"Age\" and the third \"Quality of life\".\n",
    "\n",
    "Sometimes you do not find reasonable naming for your PCs. This is entirely normal, you can proceed with your analysis regardless.\n",
    "\n",
    "Let's now see some examples ☺️\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Example 1: Psychological data\n",
    "### 1. Load required libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load your dataset and take a general look\n",
    "The data set is generated for this tutorial. However, it was not completely randomly generated. I looked at the published data from (K.Peter at al. 2017) and generated a dataset similar to it. This data set has the following variables: **id**, **pain score**, **BDI score**, **negative-view-of-self score**, **physical-function score** and **disability score**.\n",
    "First, take a look at the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'https://github.com/abdulrahman1123/analysis_examples/raw/main/dep_pain.csv'\n",
    "data = pd.read_csv(data_dir)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Normalize data\n",
    "It is important to normalize your data before doing PCA. Otherwise you will have meaningless results ... kind of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled = StandardScaler().fit_transform(data.iloc[:,1::])\n",
    "data = pd.DataFrame(data_scaled,columns = data.columns[1::])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create PCA for the entire dataset in order to determine the best number of components\n",
    "For this, we will plot the eigenvalues (AKA explained variance or relative importance) of each PC, and determine the best number of variables. The function \"plot_pca\" used here can be found in **utils.py** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = data.shape[1]\n",
    "pca_temp = PCA(n_components=n_comp)\n",
    "data_pca_temp = pca_temp.fit_transform(data)\n",
    "plot_pca(pca_temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "Take a look at the figure above and decide where you can find an \"elbow\", where the values stop descending rapidly. Based on this, a good number of PCA components is 2. Let's proceed with that. The function \"get_components\" used here can be found in **utils.py** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 2\n",
    "\n",
    "pca = PCA(n_components=n_comp)\n",
    "\n",
    "data_pca = pca.fit_transform(data)\n",
    "\n",
    "comp = get_components(pca,data.columns, True,0.35)\n",
    "\n",
    "comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figures above, we can say that we have two PCs. Looking at the \"Explained Var\" row, we see that these variables explain about 73% of the data, which is good percentage. This means that we can replace our 5 variables with only 2, and proceed with them.\n",
    "\n",
    "Additionally, we can see that the first component has its highest contribution from BDI, physical function and negative view of self, which gives an idea of what this component represents, which we can roughly call \"negative emotions\". The second component has its highest loading from pain and disability. Thus, we can call it \"pain & disability\" component. It is not crucial to understand what each component means, but it would make your components more understandable to you and to you readers. However, do not worry about giving names for PCs as they sometimes do not represent anything meaningful to us.\n",
    "\n",
    "Have a look at the final PCA data you have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Example 2: Genetics Data\n",
    "The following data is published by: [Zhang Y et al. 2021](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8762060/)\n",
    "\n",
    "It represents RNA sequencing results for 30 patients with cancer and 30 control samples (matched normal tissue, I am assuming from the same patients). The data are of the type FPKM (Fragments Per kilobase Per Million reads), which basically represents the expression level per gene. The higher the number the higher the gene's expression.\n",
    "\n",
    "Let's do the same steps as above:\n",
    "- have a look at the data\n",
    "- scale the data\n",
    "- determine the number of components that you need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "gendat = pd.read_csv('https://github.com/mmilano87/PCAPxDEG/raw/main/dataset/GSE183947_fpkm.csv')\n",
    "\n",
    "# fix the data index and remove the first column\n",
    "gendat.index = gendat['Unnamed: 0'].values\n",
    "gendat = gendat.drop('Unnamed: 0',axis = 1)\n",
    "\n",
    "# flip the data. Ultimately, we need each column to represent a gene, and each row to be one sample\n",
    "gendat = gendat.T\n",
    "\n",
    "# create a variable to represent the two subgroups in the data set. This will be helpful during plotting\n",
    "subgroup = np.array(['Control' if item.startswith('CAP') else 'Cancer' for item in list(gendat.index)])\n",
    "\n",
    "# MaxAbsScaler scales every feature based on its maximum absolute value\n",
    "trans_gendat = MaxAbsScaler().fit_transform(gendat)\n",
    "trans_gendat = pd.DataFrame(trans_gendat, index = gendat.index, columns=gendat.columns)\n",
    "\n",
    "# determine best number of components\n",
    "pca_temp = PCA(n_components=trans_gendat.shape[0])\n",
    "pca_data = pca_temp.fit_transform(trans_gendat)\n",
    "\n",
    "plot_pca(pca_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "### Perform PCA using 10 components (you can change it if you want)\n",
    "Have a look at the most important 10 genes for each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 10\n",
    "\n",
    "pca = PCA(n_components=n_comp)\n",
    "pca_data = pca.fit_transform(trans_gendat)\n",
    "pca_data = pd.DataFrame(pca_data, columns = ['PCA'+str(i+1) for i in range(n_comp)])\n",
    "pca_data.index =gendat.index\n",
    "\n",
    "pca_comp = get_components(pca,or_cols=trans_gendat.columns, plot_result=True,text_threshold=0.005, max_plot_feature=10, max_plot_comp=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the resulting dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check which of these components is relevant to our Cancer samples\n",
    "For this, we need to plot the most important PCs and see which of them are able to split the samples into Cancer vs Control. The first components usually have higher chance of doing this, as they carry more information than the rest of the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.where(subgroup=='Control','steelblue','indianred')\n",
    "\n",
    "fig = plt.figure(figsize=(11,4))\n",
    "ax = fig.add_subplot(121,projection = '3d')\n",
    "\n",
    "ax.scatter(pca_data['PCA1'],pca_data['PCA3'],pca_data['PCA2'], c=colors, s = 60, edgecolors='white', linewidth = 1)\n",
    "ax.set_xlabel('PCA1');ax.set_ylabel('PCA3');ax.set_zlabel('PCA2')\n",
    "for group,color in zip(np.unique(subgroup),['indianred','steelblue']):\n",
    "    ax.scatter(pca_data.loc[subgroup==group,'PCA1'].values[0],pca_data.loc[subgroup==group,'PCA2'].values[0],\n",
    "               pca_data.loc[subgroup==group,'PCA3'].values[0],s=60,edgecolors='white',c=color, label = group)\n",
    "plt.legend()\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.scatter(pca_data['PCA1'],pca_data['PCA3'],c = colors, edgecolor = 'white',s = 60)\n",
    "ax2.set_xlabel('PCA1');ax2.set_ylabel('PCA3')\n",
    "for group,color in zip(np.unique(subgroup),['indianred','steelblue']):\n",
    "    ax2.scatter(pca_data.loc[subgroup==group,'PCA1'].values[0],pca_data.loc[subgroup==group,'PCA3'].values[0],\n",
    "                c=color, label = group)\n",
    "plt.legend()\n",
    "\n",
    "\"\"\"ax3 = fig.add_subplot(133)\n",
    "ax3.scatter(pca_data['PCA1'],pca_data['PCA3'],c = colors, edgecolor = 'white',s = 60)\n",
    "ax3.set_xlabel('PCA1');ax3.set_ylabel('PCA3')\n",
    "for group,color in zip(np.unique(subgroup),['indianred','steelblue']):\n",
    "    ax2.scatter(pca_data.loc[subgroup==group,'PCA1'].values[0],pca_data.loc[subgroup==group,'PCA3'].values[0],\n",
    "                c=color, label = group)\n",
    "plt.legend()\n",
    "plt.subplots_adjust(wspace=0.5)\"\"\"\n",
    "''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC #3 seems to get the job done. So, in a sense, we can say that the information carried within PC #3 are relevant to differentiating cancer from control. This means that the RNA sequences you are testing carry information about cancer ... so, good job, this confirms that your choice of genetics are relevant to your problem, which is always good. But, that is not enough ... so, what else can you do with this?\n",
    "\n",
    "Maybe you can:\n",
    "- have a look at the genes that have the highest loading on PC3, consider them cancer-related and do further analysis on them\n",
    "- confirm other findings you already have using other analysis methods\n",
    "- What else?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Extra Content\n",
    "### Excercise: \n",
    "Extract the 0.2% most-expressed (99.8 percentile) and lowest-expressed (0.2 percentile) genes.\n",
    "\n",
    "Sample code is provided below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_3_dat = pca_comp['PCA3'].iloc[0:-1]\n",
    "# To extract the values above the 99.8th percentile, modify this line:\n",
    "th_999 = np.percentile(pca_3_dat,99.0)\n",
    "print(th_999)\n",
    "print(np.max(pca_3_dat))\n",
    "# For the lowest 0.1%, modify this line\n",
    "th_001 = np.percentile(pca_3_dat,1)\n",
    "highest_expressed = pca_3_dat.loc[pca_3_dat>th_999].index\n",
    "lowest_expressed = pca_3_dat.loc[pca_3_dat<th_001].index\n",
    "\n",
    "print('Genes of highest expression: ', highest_expressed)\n",
    "print('Genes of lowest expression: ', lowest_expressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What more can you do?\n",
    "\n",
    "While this might not be the best way to find the most relevant genes. It can be a start point for your anlaysis, or a confirmation of other analyses. For example, getting back to the original paper of Zhang Y et al. 2021 ([found here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8762060/)), we find that the authors identified the differentially expressed genes (DEG), which takes logFC and p-value into account. You can open [this link](https://www.ncbi.nlm.nih.gov/core/lw/2.0/html/tileshop_pmc/tileshop_pmc_inline.html?title=Click%20on%20image%20to%20zoom&p=PMC3&id=8762060_fgene-12-723477-g001.jpg) to see their results.\n",
    "\n",
    "In this analysis, they grouped the genes into two based on their expression levels and found that they group into two groups. Each group corresponds nicely with the type of sample (cancerous vs control). We can prove this by looking at the PCA3 value of the DEGs as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_group_1 = ['COL11A1','CXCL10','KPNA2','PLK1','KIFC1','E2F1','FANCA','RAD54L','FOXM1','MYBL2','HIST2H3C','HIST2H3A',\n",
    "               'HIST1H2AB','TROAP','HIST1H1B','HIST1H2AH','HIST1H3B','HIST1H2BL','KIF2C','CCNB1','CBX2','TMEM132A','HN1','TK1','H2AFX']\n",
    "gene_group_2 = ['LPHN3','MAB21L1','FAT4','RUNX1T1','SEMA6A','TSHZ2','RAI2','CACNA1G','COL4A6','GFRA1','ARHGAP6',\n",
    "               'PGM5', 'ABCA10','ABCA9','ABCA8','ALDH1A2','SPTBN4','FLG2','DES','SYNPO2','MYH11','PRDM16','MYOCD','PHYHIP']\n",
    "\n",
    "\n",
    "gene_group_1.reverse()\n",
    "gene_group_2.reverse()\n",
    "inc_genes = gene_group_2+gene_group_1\n",
    "\n",
    "\n",
    "plt_comp = pca_comp.loc[inc_genes,'PCA3']\n",
    "colors = ['indianred']*len(gene_group_2)+['steelblue']*len(gene_group_1)\n",
    "fig, ax = plt.subplots(figsize=(4, 7.5))\n",
    "ax.barh(plt_comp.index, plt_comp, color=colors, edgecolor='black', linewidth=0.75)\n",
    "ax.barh(plt_comp.index[-1], plt_comp.iloc[-1], color=colors[-1], edgecolor='black', linewidth=0.75, label = 'Gene group 1')\n",
    "ax.barh(plt_comp.index[0], plt_comp.iloc[0], color=colors[0], edgecolor='black', linewidth=0.75, label = 'Gene group 2')\n",
    "ax.set_title('Gene groups (as per the paper)\\nand their loadings on PC3')\n",
    "ax.set_xlabel('PCA3 loading', size=13)\n",
    "ax.set_ylabel('Chosen genes', size=13)\n",
    "plt.legend()\n",
    "plt.subplots_adjust(left=0.3, right=0.975, top=0.93, bottom=0.07)\n",
    "ax.set_ylim(-0.75,len(inc_genes)+3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# References and further reading\n",
    "\n",
    "[Tutorial from Builtin website](https://builtin.com/data-science/step-step-explanation-principal-component-analysis)\n",
    "\n",
    "[Machine learnign mastery website](https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/)\n",
    "\n",
    "[Prof Gregory Valiant notes](https://web.stanford.edu/class/cs168/l/l7.pdf)\n",
    "\n",
    "[Paper by Zhang Y et al. 2021](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8762060/)\n",
    "\n",
    "[Glowing python blog](https://glowingpython.blogspot.com/2011/07/pca-and-image-compression-with-numpy.html)\n",
    "\n",
    "[Saksham Gakhar blog](https://web.stanford.edu/~sakshamg/portfolio/PCA_genomes/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
