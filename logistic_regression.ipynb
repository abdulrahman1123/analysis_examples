{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Logistic Regression Example\n",
    "The general formula of linear regression is familiar to most, and is given by $Y = b_0+b_1 X_1$. Graphically, it represents a linear correlation between two variables ($Y$ and $X_1$)\n",
    "\\frac{1}{1+e^{-(b_0+b_1 X_1+b_2 X_2+...))}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, recall_score,confusion_matrix\n",
    "\n",
    "def choose_model(model, c_type = 'best', plot_result = True):\n",
    "    \"\"\"\n",
    "    This function chooses a model from a set of models identified using LogisticRegressionCV\n",
    "    It can return the best model (model.C_) or the most parsimonious model, which is the model whose score is\n",
    "    within 1 standard error from the best score\n",
    "    :param model: logistic regression model with cross validation (LogisticRegressionCV)\n",
    "    :param c_type: type of C value to return,\n",
    "                   can be either 'best' for best model, or 'par' for the most parsimonious model\n",
    "    :param plot_result: whether to plot the reult and show the best model and parsimonious model on the same figure\n",
    "    :return: C value for the chosen model\n",
    "    \"\"\"\n",
    "    n_folds = model.coefs_paths_[1.0].shape[0]\n",
    "    c_vals = model.Cs_\n",
    "    best_c = model.C_\n",
    "\n",
    "    best_c_ind = np.where(np.abs(c_vals - model.C_) < 1e-10)[0][0]\n",
    "\n",
    "    included_vars = np.sum(model.coefs_paths_[1.0].mean(axis=0) != 0,  axis=1) - 1  # the -1 is make sure the intercept is not included\n",
    "    included_vars = included_vars[\n",
    "        [int(item) for item in np.linspace(0, len(included_vars) - 1, 30)]]  # Take only 30 samples from included_vars\n",
    "    scores = model.scores_[1.0].mean(axis=0)\n",
    "    scores_sem = model.scores_[1.0].std(axis=0) / np.sqrt(n_folds)\n",
    "\n",
    "    # Get 1 standard error of the mean (SEM) from the best accuracy,\n",
    "    best_sem = scores_sem[best_c_ind]\n",
    "\n",
    "    # finds the last point where scores are within one SEM from best score\n",
    "    c1se_ind = np.where(scores[best_c_ind] - scores[0:best_c_ind] < best_sem)[0][0]\n",
    "    c1se = model.Cs_[c1se_ind]  # least acceptable score\n",
    "    if plot_result:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        ax3 = ax.twiny()\n",
    "        ax3.set_xticks(np.arange(0, len(included_vars) + 2), [''] + list(included_vars) + [''], font = 'Cambria', fontsize = 12)\n",
    "        ax3.tick_params(width=0, length = 0)\n",
    "        ax3.set_xlabel('Included Variables', font = \"Cambria\", fontsize = 18)\n",
    "        ax.axvline(x=np.log(best_c), color='grey', ls='-', lw=1, label='Best Score Model')\n",
    "        ax.axvline(x=np.log(c1se), color='grey', ls='-.', lw=1, label='Parsimonious Model')\n",
    "        ax.errorbar(np.log(model.Cs_), scores, scores_sem, fmt='o', linewidth=1,\n",
    "                    color='grey', mfc='royalblue', mec='none', capsize=4)\n",
    "        ax.legend()\n",
    "        x_axis_text = np.round(ax.get_xticks()[1:-1],1)\n",
    "        y_axis_text = np.round(ax.get_yticks()[1:-1],1)\n",
    "        ax.set_xticks(ticks =x_axis_text, labels =  x_axis_text,font = 'Cambria', fontsize = 12)\n",
    "        ax.set_yticks(ticks =y_axis_text, labels =  y_axis_text,font = 'Cambria', fontsize = 12)\n",
    "        ax.set_xlabel('log(C)',font = 'Cambria', fontsize = 18)\n",
    "        ax.set_ylabel('Accuracy',font = 'Cambria', fontsize = 18)\n",
    "\n",
    "    if c_type=='best':\n",
    "        return model.C_[0]\n",
    "    elif c_type == 'par':\n",
    "        return c1se\n",
    "    else:\n",
    "        raise Warning(\"c_type can only be set to 'best' or 'par'\")\n",
    "\n",
    "\n",
    "def model_performance(model,X_test,y_test):\n",
    "    \"\"\"\n",
    "    Print the coefficients and compute accuracy\n",
    "    :param model: the model to be tested\n",
    "    :return: print the coeffcieints and compute accuracy\n",
    "    \"\"\"\n",
    "    # let's have a look at the coefficients and see if anything was removed\n",
    "    print('Coefficients:')\n",
    "    coefs = [model.intercept_[0]] + list(model.coef_[0])\n",
    "    coefs = [str(np.round(item,3)) if item!=0 else \"-\" for item in coefs]\n",
    "    coef_names = ['intercept'] + list(data.columns[1:-1])\n",
    "    coefficients = pd.DataFrame(coefs, index=coef_names,columns=['value'])\n",
    "    print(coefficients)\n",
    "    print('\\n Scores:')\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    sens = tp / (tp + fn)\n",
    "    spec = tn / (tn + fp)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return pd.DataFrame([[sens, spec, accuracy]], columns=['Sensitivity', 'Specificity', 'Accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "### 2. Load the data, impute missing values and have a look at the data\n",
    "The data are breast cancer data. Retreived from the article [Machine learning in medicine: a practical introduction](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6425557/). It was originally publicly available from the paper [Machine Learning Repository of University of California Irvine](http://archive.ics.uci.edu/).\n",
    "\n",
    "This dataset consists of [It consists of characteristics, or features, of cell nuclei taken from breast masses which were sampled using fine-needle aspiration (FNA), a common diagnostic procedure in oncology. The clinical samples used to form this dataset were collected from January 1989 to November 1991.] (Sidey-Gibbons and Sidey-Gibbons 2019) Each sample is then classified as malignant or benign in the 'class' column (1= malignant and 0 = benign)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'https://github.com/abdulrahman1123/analysis_examples/raw/main/breast_cancer_wisconsin.csv'\n",
    "data = pd.read_csv(data_dir)\n",
    "data = data.iloc[0:150,:]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "### 3. Create training and test datasets\n",
    "\n",
    "The following generally applies for all machine learning algorithms:\n",
    "\n",
    "The first step to do is to <b>split your data into training and testing datasets</b>. The training dataset will be used to train the model and evaluate parameters, while the testing dataset will only be used at the end to test how well did the model learn.\n",
    "\n",
    "The training dataset can be further divided into training and validation datasets, where the validation dataset is used to evaluate the parameters in order to reach to the best model. The way we will be doing this is represented in the following figure from [scikit-learn website](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=440 height=305 />\n",
    "\n",
    "After splitting the data, we will impute the missing values using the mean of each column. This will be done separately for training and testing datasets.\n",
    "\n",
    "Then, the data will be normalized (mean = 0 and sd = 1) so that it can coverge faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose features\n",
    "X = data.iloc[:,1:-1]\n",
    "y = data.iloc[:,-1]\n",
    "\n",
    "# Divide into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)\n",
    "\n",
    "print(f\"Size of training dataset = {X_train.shape} and testing dataset = {X_test.shape}\")\n",
    "\n",
    "# Using the mean, impute the training and testing datasets separately\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_train = imp.fit_transform(X_train.copy())\n",
    "X_test = imp.fit_transform(X_test.copy())\n",
    "\n",
    "\n",
    "# scale both X_train and X_test for faster convergence\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "### 4. Create and fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the best model, we want to use L1-regularization. This is basically a penalty that is applied to the model to remove irrelevant features.\n",
    "\n",
    "L1-regularization basically assigns values for each of the variables to determine how sensitive the final model is to each of the variables. This regularization can be made more or less sensitive by modifying its parameter (C). Lower C values mean more sensitivity -> higher chance for variables to be removed. Higher C values mean less sensitivity and so on.\n",
    "\n",
    "The aim of using regularization at all is to remove irrelevant variables and prevent overfitting\n",
    "\n",
    "For further information, you can have a look at this [website](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c)\n",
    "\n",
    "Choose a number of c-values for the model to validate during training. The model will find the best value of C using cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a number of c-values for the model to test.\n",
    "c_vals = np.logspace(-3,1, 50) # The smaller the value, the higher the penalty is\n",
    "\n",
    "\n",
    "cv_model = LogisticRegressionCV(Cs=c_vals,penalty='l1', cv=10,\n",
    "                             tol=0.001, solver='saga', scoring='accuracy')\n",
    "# you can also choose scoring = 'neg_mean_squared_error', but remember to multiply scores with -1\n",
    "\n",
    "# fit the model to the training data set\n",
    "cv_model.fit(X_train,y_train)\n",
    "\n",
    "print (f\"Best model was found. Best C value is {cv_model.C_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "### 5. Show the results\n",
    "##### 5.1 For the best model\n",
    "A Table of coefficients will be displayed along with model performance on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance(cv_model,  X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2 For the most parsimonious model\n",
    "A Parsimonious model can be chosen by chosing a smaller C value. This C value should have accuracy that lies within one standard error of the mean (SEM) from the best accuracy achieved. Basically, you are choosing a C with very similar performance, yet with less number of variables included. This follows what is used in glmnet package for R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose parsimonious model\n",
    "par_c = choose_model(model = cv_model,c_type='par', plot_result=True)\n",
    "par_model = LogisticRegression(C=par_c, penalty='l1', tol = 0.001, solver = 'saga').fit(X_train,y_train)\n",
    "model_performance(par_model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
