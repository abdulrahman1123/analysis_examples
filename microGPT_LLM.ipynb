{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a4f013",
   "metadata": {},
   "source": [
    "# Large Language Models\n",
    "\n",
    "In our pursuit of making computers understand and generate human-like text, LLM were developed.\n",
    "Language modeling in general is concerned with predicting the next word in a sequence of words. One of the early and basic examples of LMs is the n-gram model, where the probability of a word occurring is calculated based on the previous n-1 words.\n",
    "\n",
    "For example, in a 5-gram model, and the sentence \"The quick brown fox jumps over the lazy dog\", the probability of the word \"dog\" occuring is calculated based on the previous 4 words \"over the lazy\". This is a very simple model and does not capture the context of the sentence very well. This is where LLMs come in. LLMs are able to capture the context of the sentence and generate more accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23bcee1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77bc070c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad08802d",
   "metadata": {},
   "source": [
    "---\n",
    "### Sources:\n",
    "[Introduction to Large Language Models](https://www.baeldung.com/cs/large-language-models)\n",
    "\n",
    "[Youtube video about writing a GPT from scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=2409s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e747ec88",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
