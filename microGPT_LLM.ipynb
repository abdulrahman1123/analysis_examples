{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f9a4f013",
      "metadata": {
        "id": "f9a4f013"
      },
      "source": [
        "# Large Language Models\n",
        "\n",
        "In our pursuit of making computers understand and generate human-like text, LLM were developed.\n",
        "Language modeling in general is concerned with predicting the next word in a sequence of words. One of the early and basic examples of LMs is the n-gram model, where the probability of a word occurring is calculated based on the previous n-1 words.\n",
        "\n",
        "For example, in a 5-gram model, and the sentence \"The quick brown fox jumps over the lazy dog\", the probability of the word \"dog\" occuring is calculated based on the previous 4 words \"over the lazy\". This is a very simple model and does not capture the context of the sentence very well. This is where LLMs come in. LLMs are able to capture the context of the sentence and generate more accurate predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b23bcee1",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "b23bcee1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository and all its functions\n",
        "!git clone https://github.com/abdulrahman1123/analysis_examples.git\n",
        "\n",
        "# Add the cloned directory to Python path\n",
        "import sys\n",
        "sys.path.append('/content/analysis_examples')\n",
        "\n",
        "# Import important functions, including functions to build the model and train it\n",
        "from microGPT import *"
      ],
      "metadata": {
        "id": "j7Wm1rtQqrtl",
        "outputId": "890d0e60-4095-4683-f716-6cee3cf7996e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        }
      },
      "id": "j7Wm1rtQqrtl",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'analysis_examples' already exists and is not an empty directory.\n",
            "Encoded IDs: [2, 386, 1639, 7321, 6060, 431, 402, 751, 1128, 121, 731, 6457, 2073, 3]\n",
            "Tokens: ['<BOS>', 'ĠThe', 'Ġquick', 'Ġbrown', 'Ġfox', 'Ġj', 'um', 'ps', 'Ġover', 'Ġthe', 'Ġla', 'zy', 'Ġdog', '<EOS>']\n",
            "Decoded:  The quick brown fox jumps over the lazy dog\n",
            "Vocab size: 10000\n",
            " Gre ----> 3894\n",
            "et ----> 165\n",
            "ings ----> 519\n",
            ", ----> 12\n",
            " My ----> 1308\n",
            " king ----> 734\n",
            "! ----> 4\n",
            " ----> 3\n",
            "0 141058 156732\n",
            "156732 297790 313464\n",
            "313464 454522 470196\n",
            "470196 611254 626928\n",
            "626928 767986 783660\n",
            "783660 924718 940392\n",
            "940392 1081450 1097124\n",
            "1097124 1238182 1253856\n",
            "1253856 1394914 1410588\n",
            "1410588 1551646 1567320\n",
            "train data shape:  torch.Size([1410580]) val data shape: torch.Size([156740])\n",
            "Train size = 1410580 tokens ... Validation size = 156740 tokens\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3155868857.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Import important functions, including functions to build the model and train it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmicroGPT\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/analysis_examples/microGPT.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/analysis_examples/microGPT.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, max_iters, eval_iters, train_data, val_data, batch_size, block_size, device, optimizer, scheduler)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# Evaluate periodically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             losses = estimate_loss(model, eval_iters, train_data, val_data,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "####################\n",
        "# hyperparameters\n",
        "####################\n",
        "batch_size = 32 # how many independent sequences will we process in parallel\n",
        "block_size = 256 # what is the maximum context length for predictions\n",
        "max_iters = 10000\n",
        "learning_rate = 2e-4\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") # change it to mps for Mac, and Cuda if you have Nvidia graphics\n",
        "eval_iters = 100\n",
        "eval_interval = max_iters//10\n",
        "n_embd = 256\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "n_embd = (n_embd//n_head)*n_head\n",
        "dropout = 0.1\n",
        "vocab_size = 10000\n",
        "# ------------\n",
        "\n",
        "## Download Shakespeare's work\n",
        "url1 = \"https://www.gutenberg.org/cache/epub/100/pg100.txt\"\n",
        "text = requests.get(url1).text.replace('\\r','')\n",
        "text = text[text.find('THE SONNETS\\n\\n')::] # remove text introduction\n",
        "\n",
        "#create tokens\n",
        "encode,decode, tokenizer, vocab_size = tokenize(text, vocab_size)\n",
        "\n",
        "# check what it does\n",
        "encoding = encode('Greetings, My king!')\n",
        "for item in encoding[1::]:\n",
        "    print(f\"{decode([item])} ----> {item}\")\n",
        "\n",
        "\n",
        "# Encode the entire dataset\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "\n",
        "# split into training and testing\n",
        "train_data, val_data = train_test_split(data, 0.9)\n",
        "train_data, val_data = train_data.to(device), val_data.to(device)\n",
        "print(f'Train size = {train_data.shape[0]} tokens ... Validation size = {val_data.shape[0]} tokens')\n",
        "\n",
        "# create the model, and optimize the learning process\n",
        "model = BigramLanguageModel(vocab_size, n_embd, block_size, n_head, dropout, n_layer, device).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=get_lr_lambda(max_iters=max_iters, warmup_steps=200))\n",
        "\n",
        "\n",
        "\n",
        "# train the model\n",
        "train_model(model, max_iters, eval_iters, train_data, val_data, batch_size, block_size,device,optimizer,scheduler)\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "base_dir = r\"\\\\klinik.uni-wuerzburg.de\\homedir\\userdata11\\Sawalma_A\\data\\downloads\\LLMs\"\n",
        "model_name = f'model_batch{batch_size}_vocab{vocab_size}_nembed{n_embd}_block{block_size}_nhead{n_head}_n_layer{n_layer}'\n",
        "model_path = os.path.join(base_dir,f'{model_name}.pt')\n",
        "\n",
        "#save the model\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved as: {model_name}.pt\")\n",
        "\n",
        "\n",
        "#load the model\n",
        "model = BigramLanguageModel(vocab_size, n_embd, block_size, n_head, dropout, n_layer, device)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "prompt = \"To be, or not to be \"\n",
        "input_ids = torch.tensor([tokenizer.encode(prompt).ids], dtype=torch.long, device=device)\n",
        "out = model.generate(idx=input_ids, max_new_tokens=100)\n",
        "print(tokenizer.decode(out[0].tolist()))\n"
      ],
      "metadata": {
        "id": "Hy6pLTNuq4EQ"
      },
      "id": "Hy6pLTNuq4EQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "77bc070c",
      "metadata": {
        "id": "77bc070c"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ad08802d",
      "metadata": {
        "id": "ad08802d"
      },
      "source": [
        "---\n",
        "### Sources:\n",
        "[Introduction to Large Language Models](https://www.baeldung.com/cs/large-language-models)\n",
        "\n",
        "[Youtube video about writing a GPT from scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=2409s)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e747ec88",
      "metadata": {
        "id": "e747ec88"
      },
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}