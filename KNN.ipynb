{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors\n",
    "## What is it?\n",
    "K-Nearest neighbors is a widely-used supervised machine learning algorithm used mainly for classification. It works on the assumption that similar data exist in close proximity within the feature space. When given new data, the algorithm finds the k-nearest data points (neighbors) and makes prediction based on the majority class.\n",
    "Note: It is a non-parametric method, which means that it does not have particular assumptions about the distribution of the used data.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/abdulrahman1123/analysis_examples/refs/heads/main/KNN_general.png\" width=500 height=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boudary\n",
    "\n",
    "A decision boundary is the imagenary line that separates the difference classes in question. In other words, it is the set of points at whichc the decision criterion is exactly equal for two or more classes. In order to calculate it, KNN needs to know two important things, the number of neighbors you wish to consider (K) and the distance calculation method.\n",
    "### K\n",
    "Based on the chosen number of neighbors, the model can have different performance. When K is **low**, it will lead to complex decision boundary, and possibly over fitting, while **high** K values, will lead to a smooth decision boundary that is less sensitive to single data points, which can lead to underfitting.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/abdulrahman1123/analysis_examples/refs/heads/main/KNN_DecisionAndK.png\" width=617 height=200 />\n",
    "\n",
    "### Distance Metrics\n",
    "In order to identify the nearest points, we need some distance metric. There are a number of \n",
    "\n",
    "##### Calculating Euclidean Distance\n",
    "The formula for calculating euclidean distance for the features x,y, ..., n is given as follows:\n",
    "\n",
    "$Distance (a,b) = \\sqrt{(x_a - x_b)^2+(y_a - y_b)^2 + ... +(n_a - n_b)^2}$\n",
    "\n",
    "Other distance metrics can be used, such as Manhattan distance, but euclidean distance is a good starting point, and is the default for scikit-learn KNN algorithm.\n",
    "\n",
    "------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "The Iris flower data set, introduced by Ronald Fisher in 1936, contains measurements of 50 samples from three Iris species (setosa, virginica, and versicolor). Four features—sepal length, sepal width, petal length, and petal width—were measured for each sample. Fisher used this data to develop a linear discriminant model for classifying the species. Refer to [Wikipedia](https://en.wikipedia.org/wiki/Iris_flower_data_set) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set_context(\"paper\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset, and have a general look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "irisDF = pd.DataFrame(iris['data'], columns = iris['feature_names'])\n",
    "\n",
    "irisDF.hist()\n",
    "irisDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature and target arrays \n",
    "X = iris.data[:,1:3] # Choose two columns for better visualization\n",
    "y = iris.target \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( \n",
    "             X, y, test_size = 0.2, random_state=100) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model on the training dataset and find best K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Define a parameter grid to search over\n",
    "param_grid = {'n_neighbors': [3, 5]}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_neighbors = grid_search.best_params_['n_neighbors']\n",
    "\n",
    "# Print the best parameters and the corresponding accuracy\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = best_model.predict(X_test)\n",
    "accuracy = np.average(pred_y == y_test)\n",
    "print(f\"Model Accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a mesh to plot decision boundaries\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "# Predict class for each point in the mesh\n",
    "Z = best_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "plt.contourf(xx, yy, Z, cmap=cmap_light,alpha = 0.5)\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold, edgecolor='k', s=50, label=\"Training data\")\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap_bold, edgecolor='k', s=50, marker='x', label=\"Test data\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.title(f'KNN Decision Boundaries ({best_neighbors} Neighbors)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages of KNN\n",
    "\n",
    "#### Advantages\n",
    "- Simple, easily applied and easily interpritable\n",
    "- No prior assumptions about data distribution\n",
    "- Can handle numerical or categorical data\n",
    "- Few hyperparameters to tweak (mainly, you need to choose K and the distance metric)\n",
    "\n",
    "#### Disadvantages\n",
    "- Computationally costly: All computations will need to include all training examples for every new point to be predicted\n",
    "- Prone to overfitting. Therefore, it might be used with feature selection and dimensionality reduction methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "[Geeks for Geeks #1](https://www.geeksforgeeks.org/k-nearest-neighbours/)\n",
    "\n",
    "[Geeks for Geeks #2](https://www.geeksforgeeks.org/k-nearest-neighbours/)\n",
    "\n",
    "[Scikit-Learn website](https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py)\n",
    "\n",
    "For Regression KNN: [go to this website](https://realpython.com/knn-python/#the-abalone-problem-statement)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
