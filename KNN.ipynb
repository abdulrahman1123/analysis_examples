{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors\n",
    "## What is it?\n",
    "K-Nearest neighbors is a widely-used supervised machine learning algorithm used mainly for classification. It works on the assumption that similar data exist in close proximity within the feature space. When given new data, the algorithm finds the k-nearest data points (neighbors) and makes prediction based on the majority class.\n",
    "Note: It is a non-parametric method, which means that it does not have particular assumptions about the distribution of the used data.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/abdulrahman1123/analysis_examples/refs/heads/main/KNN_general.png\" width=500 height=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boudary\n",
    "\n",
    "A decision boundary is the imagenary line that separates the difference classes in question. In other words, it is the set of points at whichc the decision criterion is exactly equal for two or more classes. In order to calculate it, KNN needs to know two important things, the number of neighbors you wish to consider (K) and the distance calculation method.\n",
    "### Effect of K\n",
    "Based on the chosen number of neighbors, the model can have different performance. When K is **low**, it will lead to complex decision boundary, and possibly over fitting, while **high** K values, will lead to a smooth decision boundary that is less sensitive to single data points, which can lead to underfitting.\n",
    "### Calculating Euclidean Distance\n",
    "The formula for calculating euclidean distance for the features x,y, ..., n is given as follows:\n",
    "\n",
    "$Distance (a,b) = \\sqrt{(x_a - x_b)^2+(y_a - y_b)^2 + ... +(n_a - n_b)^2}$\n",
    "\n",
    "Other distance metrics can be used, such as Manhattan distance, but euclidean distance is a good starting point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disadvantages\n",
    "- Computationally costly: the reason is that all computations will need to include all training examples for every new point to be predicted\n",
    "\n",
    "#### Advantages\n",
    "- Simple, easily applied and easily interpritable\n",
    "- No prior assumptions about data distribution\n",
    "- Can handle numerical or categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "https://www.geeksforgeeks.org/k-nearest-neighbours/\n",
    "https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
