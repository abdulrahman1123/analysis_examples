{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Example\n",
    "## Small Introduction\n",
    "\n",
    "The general formula of linear regression is familiar to most, and is given by $Y = b_0+b_1 X_1$. Graphically, it represents a linear correlation between two continuous variables ($Y$ and $X_1$). But, what type of formula would we need in order to describe a binary variable?\n",
    "It turns out that the best formula is logistic regression formula, which, when plotted, will have a sigmoid shape.\n",
    "Here is a figure showing the distinction between linear and logistic regression:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/abdulrahman1123/analysis_examples/main/logistic_reg_intro.png\" width=525 height=200 />\n",
    "\n",
    "\n",
    "The formula of logistic regression is closely related to that of the linear regression, and is given by:\n",
    "\n",
    "$$Probability (Y) = \\frac{1}{1+e^{-(b_0+b_1 X_1+b_2 X_2+...))}}$$\n",
    "where $b_0$ is a constant, $b_1,b_2, ...$ are coefficients multiplied by their respective variables, $X_1, X_2, ...$ are the features in the dataset (AKA columns or variables).\n",
    "\n",
    "The aim of logistic regression modeling is to find the best formula that fits the data the best (i.e., with lowest error possible).\n",
    "\n",
    "<b>Don't worry, no manual calculations will be performed, python will take care of all that. But it's good to know the basics</b>\n",
    "\n",
    "## About the Data Used\n",
    "In this notebook, I will follow the analysis mentioned in the paper [Machine learning in medicine: a practical introduction](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6425557/). The code in that paper is written in R. I translated it to python and modified it a little for the purposes of this course.\n",
    "\n",
    "We will use for this course the dataset provided in the same paper mentioned above. This dataset was originally publicly available from the paper [Machine Learning Repository of University of California Irvine](http://archive.ics.uci.edu/).\n",
    "\n",
    "This dataset [consists of characteristics, or features, of cell nuclei taken from breast masses which were sampled using fine-needle aspiration (FNA), a common diagnostic procedure in oncology. The clinical samples used to form this dataset were collected from January 1989 to November 1991.] (Sidey-Gibbons and Sidey-Gibbons 2019) Each sample is then classified as malignant or benign in the 'class' column (1= malignant and 0 = benign).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Analysis Steps:\n",
    "### 1. Import libraries and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, recall_score,confusion_matrix\n",
    "\n",
    "def choose_model(model, c_type = 'best', plot_result = True):\n",
    "    \"\"\"\n",
    "    This function chooses a model from a set of models identified using LogisticRegressionCV\n",
    "    It can return the best model (model.C_) or the most parsimonious model, which is the model whose score is\n",
    "    within 1 standard error from the best score\n",
    "    :param model: logistic regression model with cross validation (LogisticRegressionCV)\n",
    "    :param c_type: type of C value to return,\n",
    "                   can be either 'best' for best model, or 'par' for the most parsimonious model\n",
    "    :param plot_result: whether to plot the reult and show the best model and parsimonious model on the same figure\n",
    "    :return: C value for the chosen model\n",
    "    \"\"\"\n",
    "    n_folds = model.coefs_paths_[1.0].shape[0]\n",
    "    c_vals = model.Cs_\n",
    "    best_c = model.C_\n",
    "\n",
    "    best_c_ind = np.where(np.abs(c_vals - model.C_) < 1e-10)[0][0]\n",
    "\n",
    "    included_vars = np.sum(model.coefs_paths_[1.0].mean(axis=0) != 0,  axis=1) - 1  # the -1 is make sure the intercept is not included\n",
    "    included_vars = included_vars[\n",
    "        [int(item) for item in np.linspace(0, len(included_vars) - 1, 30)]]  # Take only 30 samples from included_vars\n",
    "    scores = model.scores_[1.0].mean(axis=0)\n",
    "    scores_sem = model.scores_[1.0].std(axis=0) / np.sqrt(n_folds)\n",
    "\n",
    "    # Get 1 standard error of the mean (SEM) from the best accuracy,\n",
    "    best_sem = scores_sem[best_c_ind]\n",
    "\n",
    "    # finds the last point where scores are within one SEM from best score\n",
    "    c1se_ind = np.where(scores[best_c_ind] - scores[0:best_c_ind] < best_sem)[0][0]\n",
    "    c1se = model.Cs_[c1se_ind]  # least acceptable score\n",
    "    if plot_result:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        ax3 = ax.twiny()\n",
    "        ax3.set_xticks(np.arange(0, len(included_vars) + 2), [''] + list(included_vars) + [''], font = 'Cambria', fontsize = 12)\n",
    "        ax3.tick_params(width=0, length = 0)\n",
    "        ax3.set_xlabel('Included Variables', font = \"Cambria\", fontsize = 18)\n",
    "        ax.axvline(x=np.log(best_c), color='grey', ls='-', lw=1, label='Best Score Model')\n",
    "        ax.axvline(x=np.log(c1se), color='grey', ls='-.', lw=1, label='Parsimonious Model')\n",
    "        ax.errorbar(np.log(model.Cs_), scores, scores_sem, fmt='o', linewidth=1,\n",
    "                    color='grey', mfc='royalblue', mec='none', capsize=4)\n",
    "        ax.legend()\n",
    "        x_axis_text = np.round(ax.get_xticks()[1:-1],1)\n",
    "        y_axis_text = np.round(ax.get_yticks()[1:-1],1)\n",
    "        ax.set_xticks(ticks =x_axis_text, labels =  x_axis_text,font = 'Cambria', fontsize = 12)\n",
    "        ax.set_yticks(ticks =y_axis_text, labels =  y_axis_text,font = 'Cambria', fontsize = 12)\n",
    "        ax.set_xlabel('log(C)',font = 'Cambria', fontsize = 18)\n",
    "        ax.set_ylabel('Accuracy',font = 'Cambria', fontsize = 18)\n",
    "\n",
    "    if c_type=='best':\n",
    "        return model.C_[0]\n",
    "    elif c_type == 'par':\n",
    "        return c1se\n",
    "    else:\n",
    "        raise Warning(\"c_type can only be set to 'best' or 'par'\")\n",
    "\n",
    "\n",
    "def model_performance(model,X_test,y_test):\n",
    "    \"\"\"\n",
    "    Print the coefficients and compute accuracy\n",
    "    :param model: the model to be tested\n",
    "    :return: print the coeffcieints and compute accuracy\n",
    "    \"\"\"\n",
    "    # let's have a look at the coefficients and see if anything was removed\n",
    "    print('Coefficients:')\n",
    "    coefs = [model.intercept_[0]] + list(model.coef_[0])\n",
    "    coefs = [str(np.round(item,3)) if item!=0 else \"-\" for item in coefs]\n",
    "    coef_names = ['intercept'] + list(data.columns[1:-1])\n",
    "    coefficients = pd.DataFrame(coefs, index=coef_names,columns=['value'])\n",
    "    print(coefficients)\n",
    "    print('\\n Scores:')\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    sens = tp / (tp + fn)\n",
    "    spec = tn / (tn + fp)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return pd.DataFrame([[sens, spec, accuracy]], columns=['Sensitivity', 'Specificity', 'Accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "### 2. Load the data, impute missing values and have a look at the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'https://github.com/abdulrahman1123/analysis_examples/raw/main/breast_cancer_wisconsin.csv'\n",
    "data = pd.read_csv(data_dir)\n",
    "data = data.iloc[0:150,:]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "### 3. Create training and test datasets\n",
    "\n",
    "The following generally applies for all machine learning algorithms:\n",
    "\n",
    "The first step to do is to <b>split your data into training and testing datasets</b>. The training dataset will be used to train the model and evaluate parameters, while the testing dataset will only be used at the end to test how well did the model learn.\n",
    "\n",
    "The training dataset can be further divided into training and validation datasets, where the validation dataset is used to evaluate the parameters in order to reach to the best model. The way we will be doing this is represented in the following figure from [scikit-learn website](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=440 height=305 />\n",
    "\n",
    "After splitting the data, we will impute the missing values using the mean of each column. This will be done separately for training and testing datasets.\n",
    "\n",
    "Then, the data will be normalized (mean = 0 and sd = 1) so that it can coverge faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose features\n",
    "X = data.iloc[:,1:-1]\n",
    "y = data.iloc[:,-1]\n",
    "\n",
    "# Divide into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)\n",
    "\n",
    "print(f\"Size of training dataset = {X_train.shape} and testing dataset = {X_test.shape}\")\n",
    "\n",
    "# Using the mean, impute the training and testing datasets separately\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_train = imp.fit_transform(X_train.copy())\n",
    "X_test = imp.fit_transform(X_test.copy())\n",
    "\n",
    "\n",
    "# scale both X_train and X_test for faster convergence\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "### 4. Create and fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the best model, we want to use L1-regularization. This is basically a penalty that is applied to the model to remove irrelevant features.\n",
    "\n",
    "L1-regularization basically assigns values for each of the variables to determine how sensitive the final model is to each of the variables. This regularization can be made more or less sensitive by modifying its parameter (C). Lower C values mean more sensitivity -> higher chance for variables to be removed. Higher C values mean less sensitivity and so on.\n",
    "\n",
    "The aim of using regularization at all is to remove irrelevant variables and prevent overfitting\n",
    "\n",
    "For further information, you can have a look at this [website](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c) for simplified explanation of L1 regularization.\n",
    "\n",
    "First, let's choose a number of c-values for the model to validate during training. The model will move over the values you provided to find the best value of C for the training dataset. <i>(Note: If you do not provide a list of Cs, it will choose 10 values between 1e-4 and 1e4 by default).</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a number of c-values for the model to test.\n",
    "c_vals = np.logspace(-3,1, 50) # The smaller the value, the higher the penalty is\n",
    "print (f\"There are {len(c_vals)} C-values. They range from {c_vals[0]} to {c_vals[-1]}\")\n",
    "\n",
    "cv_model = LogisticRegressionCV(Cs=c_vals,penalty='l1', cv=10, tol=0.001, solver='saga', scoring='accuracy')\n",
    "# you can also choose scoring = 'neg_mean_squared_error', but remember to multiply scores with -1\n",
    "\n",
    "\n",
    "print (\"\\nSearching for best model ...\\n\")\n",
    "# fit the model to the training data set\n",
    "cv_model.fit(X_train,y_train)\n",
    "\n",
    "print (f\"Best model was found. Best C value is {cv_model.C_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "### 5. Show the results\n",
    "##### 5.1 For the best model\n",
    "A Table of coefficients will be displayed along with model performance on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance(cv_model,  X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2 For the most parsimonious model\n",
    "A Parsimonious model can be chosen by chosing a smaller C value. This C value should have accuracy that lies within one standard error of the mean (SEM) from the best accuracy achieved. Basically, you are choosing a C with very similar performance, yet with less number of variables included. This follows what is used in glmnet package for R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose parsimonious model\n",
    "par_c = choose_model(model = cv_model,c_type='par', plot_result=True)\n",
    "par_model = LogisticRegression(C=par_c, penalty='l1', tol = 0.001, solver = 'saga').fit(X_train,y_train)\n",
    "model_performance(par_model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
